{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df209ba1",
   "metadata": {},
   "source": [
    "# üß† Manhwa Recommendation System - Final Version\n",
    "\n",
    "Notebook ini mencakup seluruh proses pembuatan sistem rekomendasi manhwa dengan dua fitur utama:\n",
    "1. Rekomendasi berdasarkan judul\n",
    "2. Rekomendasi berdasarkan keyword bebas\n",
    "\n",
    "Selain itu, notebook ini juga menggabungkan cover manhwa dari API AniList dan menyimpan hasilnya ke berbagai format (pickle dan Excel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5917fad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pickle\n",
    "import gzip\n",
    "from googletrans import Translator\n",
    "\n",
    "# Download stopwords for English\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a553c",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504439c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_excel('Manhwa_Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2a9d5",
   "metadata": {},
   "source": [
    "# Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed03f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply to necessary columns\n",
    "df['clean_synopsis'] = df['synopsis'].apply(clean_text)\n",
    "df['clean_genres'] = df['genres'].apply(clean_text)\n",
    "df['clean_authors'] = df['authors'].apply(clean_text)\n",
    "\n",
    "# Combine Tags\n",
    "df['tags'] = df['clean_synopsis'] + \". \" + df['clean_genres'] + \". \" + df['clean_authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784d38d",
   "metadata": {},
   "source": [
    "## üìê TF-IDF Vectorizer & Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813de1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuk rekomendasi by judul\n",
    "vectorizer_title = TfidfVectorizer()\n",
    "tag_matrix = vectorizer_title.fit_transform(df['tags'])\n",
    "similarity = cosine_similarity(tag_matrix)\n",
    "\n",
    "# Untuk rekomendasi by keyword\n",
    "tag_vectorizer = TfidfVectorizer()\n",
    "tag_vectors = tag_vectorizer.fit_transform(df['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d7421",
   "metadata": {},
   "source": [
    "## üì• Load Cover from API AniList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969dd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cover_url(title, cache={}):\n",
    "    if title in cache:\n",
    "        return cache[title]\n",
    "    url = 'https://graphql.anilist.co'\n",
    "    query = '''\n",
    "    query ($search: String) {\n",
    "      Media(search: $search, type: MANGA) {\n",
    "        coverImage {\n",
    "          large\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    variables = {'search': title}\n",
    "    try:\n",
    "        response = requests.post(url, json={'query': query, 'variables': variables})\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        cover = data['data']['Media']['coverImage']['large']\n",
    "        cache[title] = cover\n",
    "        return cover\n",
    "    except:\n",
    "        return 'https://via.placeholder.com/150?text=No+Image'\n",
    "\n",
    "df['cover_url'] = df['title'].apply(get_cover_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d979570",
   "metadata": {},
   "source": [
    "## üíæ Simpan Dataset dan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f9c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semua file berhasil disimpan!\n"
     ]
    }
   ],
   "source": [
    "# Save compressed pickle\n",
    "with gzip.open('manhwa_dict_with_cover.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(df[['no', 'title', 'synopsis', 'genres', 'authors', 'tags', 'cover_url']].to_dict(), f)\n",
    "\n",
    "with gzip.open('similarity.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(similarity, f)\n",
    "\n",
    "with gzip.open('tag_vectorizer.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(tag_vectorizer, f)\n",
    "\n",
    "with gzip.open('tag_vectors.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(tag_vectors, f)\n",
    "\n",
    "print(\"‚úÖ Semua file berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab527eb4",
   "metadata": {},
   "source": [
    "## Fitur Terjemahan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391e102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "def translate_to_english(text):\n",
    "    translated = translator.translate(text, src='id', dest='en')\n",
    "    return translated.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dabea9a",
   "metadata": {},
   "source": [
    "# Fungsi Rekomendasi Berdasarkan Judul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2434201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_title(selected_title):\n",
    "    idx = df[df['title'] == selected_title].index[0]\n",
    "    distances = similarity[idx]\n",
    "    manhwa_list = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:6]\n",
    "    \n",
    "    results = []\n",
    "    for i in manhwa_list:\n",
    "        row = df.iloc[i[0]]\n",
    "        results.append((row['title'], row['cover_url']))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc7c92",
   "metadata": {},
   "source": [
    "# Fungsi Rekomendasi Berdasarkan Keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e4ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_keyword(user_input):\n",
    "    # Translate user input to English if necessary\n",
    "    if is_indonesian(user_input):\n",
    "        user_input = translate_to_english(user_input)\n",
    "    \n",
    "    user_vec = tag_vectorizer.transform([user_input])\n",
    "    scores = cosine_similarity(user_vec, tag_vectors).flatten()\n",
    "    top_indices = scores.argsort()[::-1][:5]\n",
    "    \n",
    "    results = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
